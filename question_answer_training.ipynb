{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import DistilBertForQuestionAnswering\n",
    "from transformers import BertConfig\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"answers.pkl\", \"rb\") as input_file:\n",
    "    question_answers = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(question_answers) > 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('when is the last episode of season 8 of the walking dead?',\n",
       "  'March 18 , 2018',\n",
       "  {'answer_start': 193, 'answer_end': 208},\n",
       "  \"109 10 `` The Lost and the Plunderers '' TBA TBA March 4 , 2018 ( 2018 - 03 - 04 ) TBD 110 11 `` Dead or Alive Or '' TBA TBA March 11 , 2018 ( 2018 - 03 - 11 ) TBD 111 12 `` The Key '' TBA TBA March 18 , 2018 ( 2018 - 03 - 18 ) TBD Release ( edit ) The first trailer for the season was released on July 21 , 2017 , at San Diego Comic - Con\"),\n",
       " ('in greek mythology who was the goddess of spring growth?',\n",
       "  \"Persephone ( / pərˈsɛfəni / ; Greek : Περσεφόνη ) , also called Kore ( / ˈkɔːriː / ; `` the maiden '' )\",\n",
       "  {'answer_start': 1437, 'answer_end': 1540},\n",
       "  \"Part of a series on Ancient Greek religion Features ( show ) Greek mythology Ancient Greek philosophy Hellenistic philosophy Ancient Greek religion Polytheism Henosis Monism Pantheism Orthopraxy Godheads ( show ) Olympians Aphrodite Apollo Ares Artemis Athena Demeter Dionysus Hades Hephaestus Hera Hermes Hestia Poseidon Zeus Primordial deities Aether Aion Ananke Chaos Chronos Erebus Eros Gaia Hemera Nyx Phanes Pontus Thalassa Tartarus Uranus Lesser deities Alpheus Amphitrite Asclepius Bia Circe Deimos Eileithyia Enyo Eos Eris Harmonia Hebe Hecate Helios Heracles Iris Kratos Leto Metis Momus Morpheus Nemesis Nike Pan Persephone Phantasos Phobos Proteus Scamander Selene Thanatos Thetis Triton Zelus Ethics ( show ) Arete Hubris Xenia Ethic of reciprocity Practices ( show ) Amphidromia Animal sacrifice Funeral practices Greek hero cult Hieros gamos Iatromantis Libations Oracles Pharmakos Temples Votive offerings Festivals Daphnephoria Dionysia Dionysian Mysteries Eleusinian Mysteries Panathenaic Games Panhellenic Games ( Isthmian Nemean Olympic Pythian ) Thesmophoria Sacred places ( show ) Eleusis Delphi Delos Dodona Mount Olympus Olympia Texts ( show ) Argonautica Bibliotheca Corpus Hermeticum Delphic maxims Dionysiaca Epic Cycle Homeric Hymns Iliad Odyssey Orphic Hymns Theogony Works and Days History ( show ) Mycenaean gods Decline of Hellenistic polytheism Julian restoration Hellenismos portal In Greek mythology , Persephone ( / pərˈsɛfəni / ; Greek : Περσεφόνη ) , also called Kore ( / ˈkɔːriː / ; `` the maiden '' ) , is the daughter of Zeus and Demeter and is the queen of the underworld\"),\n",
       " ('what is the name of the most important jewish text?',\n",
       "  'the Shulchan Aruch',\n",
       "  {'answer_start': 128, 'answer_end': 146},\n",
       "  ') Over time , as practices develop , codes of Jewish law are written that are based on the responsa ; the most important code , the Shulchan Aruch , largely determines Orthodox religious practice today'),\n",
       " (\"what is the name of spain's most famous soccer team?\",\n",
       "  'Real Madrid',\n",
       "  {'answer_start': 86, 'answer_end': 97},\n",
       "  \"Football in Spain - wikipedia Football in Spain Football in Spain Santiago Bernabeu , Real Madrid 's Stadium\"),\n",
       " ('when was the first robot used in surgery?',\n",
       "  '1983',\n",
       "  {'answer_start': 138, 'answer_end': 142},\n",
       "  'History ( edit ) The first robot to assist in surgery was the Arthrobot , which was developed and used for the first time in Vancouver in 1983'),\n",
       " (\"who sings the song i don't care i love it?\",\n",
       "  'Icona Pop and Charli XCX',\n",
       "  {'answer_start': 27, 'answer_end': 51},\n",
       "  \"The song went on to become Icona Pop and Charli XCX 's first US hit , peaking at number seven on the Billboard Hot 100 and was certified double - platinum by the Recording Industry Association of America , denoting over 2 million copies sold in the United States\"),\n",
       " ('who are uncle owen and aunt beru related to?',\n",
       "  'Anakin',\n",
       "  {'answer_start': 36, 'answer_end': 42},\n",
       "  'Vader , in his previous identity as Anakin Skywalker , is a lead character in the prequel film trilogy'),\n",
       " ('who are uncle owen and aunt beru related to?',\n",
       "  'Padmé Amidala',\n",
       "  {'answer_start': 2, 'answer_end': 15},\n",
       "  '3 Padmé Amidala 2'),\n",
       " ('who are uncle owen and aunt beru related to?',\n",
       "  'Shmi',\n",
       "  {'answer_start': 2, 'answer_end': 6},\n",
       "  '1 Shmi Skywalker - Lars 2'),\n",
       " ('where is zimbabwe located in the world map?',\n",
       "  'in southern Africa , between the Zambezi and Limpopo Rivers , bordered by South Africa , Botswana , Zambia and Mozambique',\n",
       "  {'answer_start': 101, 'answer_end': 222},\n",
       "  'Zimbabwe ( / zɪmˈbɑːbweɪ / ) , officially the Republic of Zimbabwe , is a landlocked country located in southern Africa , between the Zambezi and Limpopo Rivers , bordered by South Africa , Botswana , Zambia and Mozambique')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train / valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = len(question_answers) // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = question_answers[:-val_size]\n",
    "valid_data = question_answers[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(train_data) + len(valid_data) == len(question_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('when is the last episode of season 8 of the walking dead?',\n",
       "  'March 18 , 2018',\n",
       "  {'answer_start': 193, 'answer_end': 208},\n",
       "  \"109 10 `` The Lost and the Plunderers '' TBA TBA March 4 , 2018 ( 2018 - 03 - 04 ) TBD 110 11 `` Dead or Alive Or '' TBA TBA March 11 , 2018 ( 2018 - 03 - 11 ) TBD 111 12 `` The Key '' TBA TBA March 18 , 2018 ( 2018 - 03 - 18 ) TBD Release ( edit ) The first trailer for the season was released on July 21 , 2017 , at San Diego Comic - Con\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(data):\n",
    "    return [d[0] for d in data], [d[2] for d in data], [d[3] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions, train_answers, train_contexts = transform_dataframe(train_data)\n",
    "valid_questions, valid_answers, valid_contexts = transform_dataframe(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': 193, 'answer_end': 208},\n",
       " {'answer_start': 1437, 'answer_end': 1540},\n",
       " {'answer_start': 128, 'answer_end': 146},\n",
       " {'answer_start': 86, 'answer_end': 97},\n",
       " {'answer_start': 138, 'answer_end': 142}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"109 10 `` The Lost and the Plunderers '' TBA TBA March 4 , 2018 ( 2018 - 03 - 04 ) TBD 110 11 `` Dead or Alive Or '' TBA TBA March 11 , 2018 ( 2018 - 03 - 11 ) TBD 111 12 `` The Key '' TBA TBA March 18 , 2018 ( 2018 - 03 - 18 ) TBD Release ( edit ) The first trailer for the season was released on July 21 , 2017 , at San Diego Comic - Con\",\n",
       " \"Part of a series on Ancient Greek religion Features ( show ) Greek mythology Ancient Greek philosophy Hellenistic philosophy Ancient Greek religion Polytheism Henosis Monism Pantheism Orthopraxy Godheads ( show ) Olympians Aphrodite Apollo Ares Artemis Athena Demeter Dionysus Hades Hephaestus Hera Hermes Hestia Poseidon Zeus Primordial deities Aether Aion Ananke Chaos Chronos Erebus Eros Gaia Hemera Nyx Phanes Pontus Thalassa Tartarus Uranus Lesser deities Alpheus Amphitrite Asclepius Bia Circe Deimos Eileithyia Enyo Eos Eris Harmonia Hebe Hecate Helios Heracles Iris Kratos Leto Metis Momus Morpheus Nemesis Nike Pan Persephone Phantasos Phobos Proteus Scamander Selene Thanatos Thetis Triton Zelus Ethics ( show ) Arete Hubris Xenia Ethic of reciprocity Practices ( show ) Amphidromia Animal sacrifice Funeral practices Greek hero cult Hieros gamos Iatromantis Libations Oracles Pharmakos Temples Votive offerings Festivals Daphnephoria Dionysia Dionysian Mysteries Eleusinian Mysteries Panathenaic Games Panhellenic Games ( Isthmian Nemean Olympic Pythian ) Thesmophoria Sacred places ( show ) Eleusis Delphi Delos Dodona Mount Olympus Olympia Texts ( show ) Argonautica Bibliotheca Corpus Hermeticum Delphic maxims Dionysiaca Epic Cycle Homeric Hymns Iliad Odyssey Orphic Hymns Theogony Works and Days History ( show ) Mycenaean gods Decline of Hellenistic polytheism Julian restoration Hellenismos portal In Greek mythology , Persephone ( / pərˈsɛfəni / ; Greek : Περσεφόνη ) , also called Kore ( / ˈkɔːriː / ; `` the maiden '' ) , is the daughter of Zeus and Demeter and is the queen of the underworld\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when is the last episode of season 8 of the walking dead?',\n",
       " 'in greek mythology who was the goddess of spring growth?',\n",
       " 'what is the name of the most important jewish text?',\n",
       " \"what is the name of spain's most famous soccer team?\",\n",
       " 'when was the first robot used in surgery?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        # if None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, valid_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NaturalQuestionsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = NaturalQuestionsDataset(train_encodings)\n",
    "val_dataset = NaturalQuestionsDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained( PRE_TRAINED_MODEL_NAME, output_hidden_states=True)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1\n",
    "MODEL_PATH = f'./google_nq_{PRE_TRAINED_MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_gpu_train_batch_size=8,\n",
    "    per_gpu_eval_batch_size=8,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_first_step=True,\n",
    "    save_steps=5000,\n",
    "    evaluate_during_training=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/gilf/huggingface\" target=\"_blank\">https://app.wandb.ai/gilf/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/gilf/huggingface/runs/3nez2wb3\" target=\"_blank\">https://app.wandb.ai/gilf/huggingface/runs/3nez2wb3</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=metric,\n",
    "    prediction_loss_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "eval_data_loader = trainer.get_eval_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/opt/anaconda3/envs/huggingface/lib/python3.7/site-packages/amp_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceISt7complexIdEEEPKNS_6detail12TypeMetaDataEv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b90731e737442c6b4e2536ffb17c3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c34c581ba24cbda92e2d84e1790b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=2938.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "/opt/anaconda3/envs/huggingface/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954c3e6eb9884a9a87d4fb999b6875e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=327.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995d9bf1a16948ae9f3b089c9d0ef7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=327.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "\n",
      "\n",
      "CPU times: user 6min 45s, sys: 1.71 s, total: 6min 47s\n",
      "Wall time: 6min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2938, training_loss=1.872358276445984)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "vl_dl = trainer.get_eval_dataloader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tensor = (next(iter(vl_dl)))\n",
    "sample_tensor.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8e6752525248f7a5cfaf3103409431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=327.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate()\n",
    "\n",
    "output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "if trainer.is_world_master():\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key, value in result.items():\n",
    "            logger.info(\"  %s = %s\", key, value)\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "# For convenience, we also re-save the tokenizer to the same directory,\n",
    "# so that you can share your model easily on huggingface.co/models =)\n",
    "if trainer.is_world_master():\n",
    "    tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
